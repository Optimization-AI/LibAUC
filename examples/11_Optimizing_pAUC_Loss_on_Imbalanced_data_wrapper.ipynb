{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK-vJ2OINH0_"
      },
      "source": [
        "# **Optimizing Partial AUC Loss on Imbalanaced Dataset**\n",
        "\n",
        "**Author**: Zhuoning Yuan\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "In this tutorial, you will learn how to quickly train a ResNet18 model by optimizing **Partial AUC (pAUC)** score using our novel optimization methods on an binary image classification task on Cifar10. This is a **wrapper** of original implementations of partial AUC losses. For orginal tutorials, please refer to  [SOPA](https://github.com/Optimization-AI/LibAUC/blob/main/examples/11_Optimizing_pAUC_Loss_with_SOPA_on_Imbalanced_data.ipynb), [SOPA-s](https://github.com/Optimization-AI/LibAUC/blob/main/examples/11_Optimizing_pAUC_Loss_with_SOPAs_on_Imbalanced_data.ipynb), [SOTA-s](https://github.com/Optimization-AI/LibAUC/blob/main/examples/11_Optimizing_pAUC_Loss_with_SOTAs_on_Imbalanced_data.ipynb). After completion of this tutorial, you should be able to use LibAUC to train your own models on your own datasets.\n",
        "\n",
        "**Useful Resources**\n",
        "\n",
        "* Website: https://libauc.org\n",
        "* Github: https://github.com/Optimization-AI/LibAUC\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "If you find this tutorial helpful in your work,  please acknowledge our library and cite the following papers:\n",
        "\n",
        "<pre>\n",
        "@article{zhu2022auc,\n",
        "  title={When AUC meets DRO: Optimizing Partial AUC for Deep Learning with Non-Convex Convergence Guarantee},\n",
        "  author={Zhu, Dixian and Li, Gang and Wang, Bokun and Wu, Xiaodong and Yang, Tianbao},\n",
        "  journal={arXiv preprint arXiv:2203.00176},\n",
        "  year={2022}\n",
        "}\n",
        "\n",
        "@misc{libauc2022,\n",
        "\ttitle={LibAUC: A Deep Learning Library for X-risk Optimization.},\n",
        "\tauthor={Zhuoning Yuan, Zi-Hao Qiu, Gang Li, Dixian Zhu, Zhishuai Guo, Quanqi Hu, Bokun Wang, Qi Qi, Yongjian Zhong, Tianbao Yang},\n",
        "\tyear={2022}\n",
        "\t}\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efaf1543"
      },
      "source": [
        "## **Installing LibAUC**\n",
        "\n",
        "Let's start with install our library here. In this tutorial, we will use beta version `1.1.9rc4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw6REpfxjMe6"
      },
      "outputs": [],
      "source": [
        "!pip install libauc==1.1.9rc4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN7i0wt8j5ex"
      },
      "source": [
        "## **Importing LibAUC**\n",
        "\n",
        "Import required libraries to use\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f31mohXAj5ex"
      },
      "outputs": [],
      "source": [
        "from libauc.models import resnet18\n",
        "from libauc.datasets import CIFAR10\n",
        "from libauc.losses.auc import pAUCLoss  # default: SOPA\n",
        "from libauc.optimizers import SOPA\n",
        "from libauc.utils import ImbalancedDataGenerator\n",
        "from libauc.sampler import DualSampler\n",
        "from libauc.metrics import pauc_roc_score\n",
        "\n",
        "import torch \n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Dataset\n",
        "import torch \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdlU4semj5ez"
      },
      "source": [
        "## **Reproducibility**\n",
        "\n",
        "These functions limit the number of sources of randomness behaviors, such as model intialization, data shuffling, etcs. However, completely reproducible results are not guaranteed across PyTorch releases [[Ref]](https://pytorch.org/docs/stable/notes/randomness.html#:~:text=Completely%20reproducible%20results%20are%20not,even%20when%20using%20identical%20seeds.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLZ0yKfuj5ez"
      },
      "outputs": [],
      "source": [
        "def set_all_seeds(SEED):\n",
        "    # REPRODUCIBILITY\n",
        "    torch.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a4eeTu5uHgX"
      },
      "source": [
        "## **Image Dataset**\n",
        "\n",
        "\n",
        "Now that we defined the data input pipeline such as data augmentations. In this tutorials, we use `RandomCrop`, `RandomHorizontalFlip`. The `pos_index_map` helps map global index to local index for reducing memory cost in loss function since we only need to track the indices for positive samples. Please refer to original paper [here](https://arxiv.org/pdf/2203.00176.pdf) for more details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do3FofgWuHgX"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, images, targets, image_size=32, crop_size=30, mode='train'):\n",
        "       self.images = images.astype(np.uint8)\n",
        "       self.targets = targets\n",
        "       self.mode = mode\n",
        "       self.transform_train = transforms.Compose([                                                \n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.RandomCrop((crop_size, crop_size), padding=None),\n",
        "                              transforms.RandomHorizontalFlip(),\n",
        "                              transforms.Resize((image_size, image_size)),\n",
        "                              ])\n",
        "       self.transform_test = transforms.Compose([\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Resize((image_size, image_size)),\n",
        "                              ])\n",
        "       \n",
        "       # for loss function\n",
        "       self.pos_indices = np.flatnonzero(targets==1)\n",
        "       self.pos_index_map = {}\n",
        "       for i, idx in enumerate(self.pos_indices):\n",
        "           self.pos_index_map[idx] = i\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        target = self.targets[idx]\n",
        "        image = Image.fromarray(image.astype('uint8'))\n",
        "        if self.mode == 'train':\n",
        "            idx = self.pos_index_map[idx] if idx in self.pos_indices else -1\n",
        "            image = self.transform_train(image)\n",
        "        else:\n",
        "            image = self.transform_test(image)\n",
        "        return image, target, int(idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfDLXy7xuMYz"
      },
      "source": [
        "## **Hyper-parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zYizokauKCk"
      },
      "outputs": [],
      "source": [
        "# general params\n",
        "lr = 1e-3\n",
        "weight_decay = 5e-4\n",
        "total_epoch = 60\n",
        "decay_epochs = [20, 40]\n",
        "batch_size = 64\n",
        "\n",
        "# By default, we use one-way partial AUC\n",
        "alpha = 0.  # a: min_tpr=0. This is fixed value (for reference only)\n",
        "beta = 0.1  # b: max_fpr=0.1\n",
        "\n",
        "# By default, pAUCLoss calls SOPA in the backend\n",
        "margin = 1.0\n",
        "eta = 1e1 # learning rate for control negative samples weights\n",
        "\n",
        "# sampling parameters\n",
        "sampling_rate = 0.5\n",
        "num_pos = int(batch_size*sampling_rate)\n",
        "num_neg = int(batch_size*(1-sampling_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6100c682"
      },
      "source": [
        "## **Loading datasets**\n",
        "\n",
        "In this step, , we will use the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) as benchmark dataset. Before importing data to `dataloader`, we construct imbalanced version for CIFAR10 by `ImbalancedDataGenerator`. Specifically, it first randomly splits the training data by class ID (e.g., 10 classes) into two even portions as the positive and negative classes, and then it randomly removes some samples from the positive class to make\n",
        "it imbalanced. We keep the testing set untouched. We refer `imratio` to the ratio of number of positive examples to number of all examples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wMVjA4UGpm_",
        "outputId": "cdca805d-e3fc-4afe-f7c6-b915b34c5007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "#SAMPLES: [31250], POS:NEG: [6250 : 25000], POS RATIO: 0.2000\n",
            "#SAMPLES: [10000], POS:NEG: [5000 : 5000], POS RATIO: 0.5000\n"
          ]
        }
      ],
      "source": [
        "train_data, train_targets = CIFAR10(root='./data', train=True)\n",
        "test_data, test_targets  = CIFAR10(root='./data', train=False)\n",
        "\n",
        "imratio = 0.2\n",
        "generator = ImbalancedDataGenerator(shuffle=True, verbose=True, random_seed=0)\n",
        "(train_images, train_labels) = generator.transform(train_data, train_targets, imratio=imratio)\n",
        "(test_images, test_labels) = generator.transform(test_data, test_targets, imratio=0.5) \n",
        "\n",
        "trainDataset = ImageDataset(train_images, train_labels)\n",
        "testDataset = ImageDataset(test_images, test_labels, mode='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P4S0T5ivlMs"
      },
      "source": [
        "Now, we can import data and load the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNAGxEe_vgx5"
      },
      "outputs": [],
      "source": [
        "trainSet = ImageDataset(train_images, train_labels)\n",
        "testSet = ImageDataset(test_images, test_labels, mode='test')\n",
        "\n",
        "sampler = DualSampler(trainSet, batch_size, sampling_rate=sampling_rate)\n",
        "trainloader = torch.utils.data.DataLoader(trainSet, batch_size=batch_size,  sampler=sampler,  shuffle=False,  num_workers=1)\n",
        "testloader = torch.utils.data.DataLoader(testSet , batch_size=batch_size, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou0d9hhvWJqk"
      },
      "source": [
        "## **Model and Loss setup**\n",
        "For `pAUCLoss`, you can set `backend='SOPAs, SOPA, SOTA-s`. Here is the breif summary for each loss below. For more details regarding the parameters, please refer to original tutorials. \n",
        "\n",
        "\n",
        "- **SOPA** [[Ref](https://github.com/Optimization-AI/LibAUC/blob/main/examples/11_Optimizing_pAUC_Loss_with_SOPA_on_Imbalanced_data.ipynb)]\n",
        "```\n",
        "Loss = pAUC_CVaR_Loss(pos_length=sampler.pos_len, num_neg=num_neg, beta=beta)\n",
        "optimizer = SOPA(model, loss_fn=loss_fn, mode='adam', lr=lr, eta=eta, weight_decay=weight_decay)\n",
        "```\n",
        "\n",
        "- **SOPAs** [[Ref](https://github.com/Optimization-AI/LibAUC/blob/main/examples/11_Optimizing_pAUC_Loss_with_SOPAs_on_Imbalanced_data.ipynb)]\n",
        "```\n",
        "loss_fn = pAUC_DRO_Loss(pos_len=sampler.pos_len, margin=margin, beta=beta, Lambda=Lambda)\n",
        "optimizer = SOPAs(model, loss_fn=loss_fn, mode='adam', lr=lr, weight_decay=weight_decay)\n",
        "```\n",
        "\n",
        "- **SOTAs** [[Ref](https://github.com/Optimization-AI/LibAUC/blob/main/examples/11_Optimizing_pAUC_Loss_with_SOTAs_on_Imbalanced_data.ipynb)]\n",
        "```\n",
        "Loss = tpAUC_KL_Loss(pos_len=sampler.pos_len, Lambda=Lambda, tau=tau)\n",
        "optimizer = SOTAs(model, loss_fn=loss_fn, mode='adam', lr=lr, gammas=(gamma0, gamma1), weight_decay=weight_decay) \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08370a67",
        "outputId": "e7537311-787c-455a-be62-a162de3da536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backend loss: SOPA\n"
          ]
        }
      ],
      "source": [
        "seed = 123\n",
        "set_all_seeds(seed)\n",
        "model = resnet18(pretrained=False, num_classes=1, last_activation=None) \n",
        "model = model.cuda()\n",
        "\n",
        "loss_fn = pAUCLoss(pos_len=sampler.pos_len, backend='SOPA', beta=beta, num_neg=num_neg, margin=margin)\n",
        "optimizer = SOPA(model.parameters(), loss_fn=loss_fn.loss_fn, mode='adam', lr=lr, eta=eta, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c6d316"
      },
      "source": [
        "## **Training**\n",
        "\n",
        "Now we start training the model. We evaluate partial AUC performance with True Positive Rate (TPR) equal to 0 (`alpha=0`) and False Positive Rate (FPR) less than or equal to 0.3 (`beta=0.3`). This can be done by `libauc.metrics.pauc_roc_score(y_true, y_pred, max_fpr=0.3, min_tpr=0)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "78d47d7d",
        "scrolled": true,
        "outputId": "fe1b125f-26c8-46a2-9c83-a02a90a022e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "------------------------------\n",
            "epoch: 0, lr: 0.0010, train_pauc: 0.6013, test_pauc: 0.6739, test_best: 0.6739\n",
            "epoch: 1, lr: 0.0010, train_pauc: 0.7130, test_pauc: 0.7297, test_best: 0.7297\n",
            "epoch: 2, lr: 0.0010, train_pauc: 0.7643, test_pauc: 0.7690, test_best: 0.7690\n",
            "epoch: 3, lr: 0.0010, train_pauc: 0.8009, test_pauc: 0.7667, test_best: 0.7690\n",
            "epoch: 4, lr: 0.0010, train_pauc: 0.8294, test_pauc: 0.7836, test_best: 0.7836\n",
            "epoch: 5, lr: 0.0010, train_pauc: 0.8523, test_pauc: 0.8172, test_best: 0.8172\n",
            "epoch: 6, lr: 0.0010, train_pauc: 0.8672, test_pauc: 0.8204, test_best: 0.8204\n",
            "epoch: 7, lr: 0.0010, train_pauc: 0.8866, test_pauc: 0.8356, test_best: 0.8356\n",
            "epoch: 8, lr: 0.0010, train_pauc: 0.9022, test_pauc: 0.8321, test_best: 0.8356\n",
            "epoch: 9, lr: 0.0010, train_pauc: 0.9130, test_pauc: 0.8010, test_best: 0.8356\n",
            "epoch: 10, lr: 0.0010, train_pauc: 0.9225, test_pauc: 0.8252, test_best: 0.8356\n",
            "epoch: 11, lr: 0.0010, train_pauc: 0.9324, test_pauc: 0.8377, test_best: 0.8377\n",
            "epoch: 12, lr: 0.0010, train_pauc: 0.9411, test_pauc: 0.8398, test_best: 0.8398\n",
            "epoch: 13, lr: 0.0010, train_pauc: 0.9478, test_pauc: 0.8385, test_best: 0.8398\n",
            "epoch: 14, lr: 0.0010, train_pauc: 0.9502, test_pauc: 0.8455, test_best: 0.8455\n",
            "epoch: 15, lr: 0.0010, train_pauc: 0.9541, test_pauc: 0.8369, test_best: 0.8455\n",
            "epoch: 16, lr: 0.0010, train_pauc: 0.9572, test_pauc: 0.8250, test_best: 0.8455\n",
            "epoch: 17, lr: 0.0010, train_pauc: 0.9602, test_pauc: 0.8436, test_best: 0.8455\n",
            "epoch: 18, lr: 0.0010, train_pauc: 0.9633, test_pauc: 0.8046, test_best: 0.8455\n",
            "epoch: 19, lr: 0.0010, train_pauc: 0.9651, test_pauc: 0.8421, test_best: 0.8455\n",
            "Reducing lr to 0.00010 @ T=15620!\n",
            "Reducing loss coefficient (moving average) to 1.00000 @ T=15620!!\n",
            "epoch: 20, lr: 0.0001, train_pauc: 0.9853, test_pauc: 0.8707, test_best: 0.8707\n",
            "epoch: 21, lr: 0.0001, train_pauc: 0.9907, test_pauc: 0.8708, test_best: 0.8708\n",
            "epoch: 22, lr: 0.0001, train_pauc: 0.9928, test_pauc: 0.8712, test_best: 0.8712\n",
            "epoch: 23, lr: 0.0001, train_pauc: 0.9941, test_pauc: 0.8702, test_best: 0.8712\n",
            "epoch: 24, lr: 0.0001, train_pauc: 0.9954, test_pauc: 0.8717, test_best: 0.8717\n",
            "epoch: 25, lr: 0.0001, train_pauc: 0.9957, test_pauc: 0.8709, test_best: 0.8717\n",
            "epoch: 26, lr: 0.0001, train_pauc: 0.9962, test_pauc: 0.8708, test_best: 0.8717\n",
            "epoch: 27, lr: 0.0001, train_pauc: 0.9964, test_pauc: 0.8698, test_best: 0.8717\n",
            "epoch: 28, lr: 0.0001, train_pauc: 0.9968, test_pauc: 0.8692, test_best: 0.8717\n",
            "epoch: 29, lr: 0.0001, train_pauc: 0.9971, test_pauc: 0.8705, test_best: 0.8717\n",
            "epoch: 30, lr: 0.0001, train_pauc: 0.9975, test_pauc: 0.8706, test_best: 0.8717\n",
            "epoch: 31, lr: 0.0001, train_pauc: 0.9975, test_pauc: 0.8704, test_best: 0.8717\n",
            "epoch: 32, lr: 0.0001, train_pauc: 0.9977, test_pauc: 0.8715, test_best: 0.8717\n",
            "epoch: 33, lr: 0.0001, train_pauc: 0.9977, test_pauc: 0.8698, test_best: 0.8717\n",
            "epoch: 34, lr: 0.0001, train_pauc: 0.9977, test_pauc: 0.8720, test_best: 0.8720\n",
            "epoch: 35, lr: 0.0001, train_pauc: 0.9979, test_pauc: 0.8684, test_best: 0.8720\n",
            "epoch: 36, lr: 0.0001, train_pauc: 0.9980, test_pauc: 0.8713, test_best: 0.8720\n",
            "epoch: 37, lr: 0.0001, train_pauc: 0.9982, test_pauc: 0.8720, test_best: 0.8720\n",
            "epoch: 38, lr: 0.0001, train_pauc: 0.9982, test_pauc: 0.8681, test_best: 0.8720\n",
            "epoch: 39, lr: 0.0001, train_pauc: 0.9981, test_pauc: 0.8716, test_best: 0.8720\n",
            "Reducing lr to 0.00001 @ T=31240!\n",
            "Reducing loss coefficient (moving average) to 0.10000 @ T=31240!!\n",
            "epoch: 40, lr: 0.0000, train_pauc: 0.9986, test_pauc: 0.8726, test_best: 0.8726\n",
            "epoch: 41, lr: 0.0000, train_pauc: 0.9986, test_pauc: 0.8713, test_best: 0.8726\n",
            "epoch: 42, lr: 0.0000, train_pauc: 0.9988, test_pauc: 0.8719, test_best: 0.8726\n",
            "epoch: 43, lr: 0.0000, train_pauc: 0.9987, test_pauc: 0.8722, test_best: 0.8726\n",
            "epoch: 44, lr: 0.0000, train_pauc: 0.9988, test_pauc: 0.8722, test_best: 0.8726\n",
            "epoch: 45, lr: 0.0000, train_pauc: 0.9987, test_pauc: 0.8732, test_best: 0.8732\n",
            "epoch: 46, lr: 0.0000, train_pauc: 0.9987, test_pauc: 0.8724, test_best: 0.8732\n",
            "epoch: 47, lr: 0.0000, train_pauc: 0.9986, test_pauc: 0.8732, test_best: 0.8732\n",
            "epoch: 48, lr: 0.0000, train_pauc: 0.9989, test_pauc: 0.8731, test_best: 0.8732\n",
            "epoch: 49, lr: 0.0000, train_pauc: 0.9988, test_pauc: 0.8726, test_best: 0.8732\n",
            "epoch: 50, lr: 0.0000, train_pauc: 0.9988, test_pauc: 0.8730, test_best: 0.8732\n",
            "epoch: 51, lr: 0.0000, train_pauc: 0.9989, test_pauc: 0.8725, test_best: 0.8732\n",
            "epoch: 52, lr: 0.0000, train_pauc: 0.9989, test_pauc: 0.8721, test_best: 0.8732\n",
            "epoch: 53, lr: 0.0000, train_pauc: 0.9989, test_pauc: 0.8722, test_best: 0.8732\n",
            "epoch: 54, lr: 0.0000, train_pauc: 0.9989, test_pauc: 0.8728, test_best: 0.8732\n",
            "epoch: 55, lr: 0.0000, train_pauc: 0.9990, test_pauc: 0.8726, test_best: 0.8732\n",
            "epoch: 56, lr: 0.0000, train_pauc: 0.9990, test_pauc: 0.8753, test_best: 0.8753\n",
            "epoch: 57, lr: 0.0000, train_pauc: 0.9990, test_pauc: 0.8744, test_best: 0.8753\n",
            "epoch: 58, lr: 0.0000, train_pauc: 0.9992, test_pauc: 0.8731, test_best: 0.8753\n",
            "epoch: 59, lr: 0.0000, train_pauc: 0.9990, test_pauc: 0.8719, test_best: 0.8753\n"
          ]
        }
      ],
      "source": [
        "print ('Start Training')\n",
        "print ('-'*30)\n",
        "test_best = 0\n",
        "train_list, test_list = [], []\n",
        "for epoch in range(total_epoch):\n",
        "    \n",
        "    if epoch in decay_epochs:\n",
        "        optimizer.update_lr(decay_factor=10, coef_decay_factor=10)\n",
        "            \n",
        "    train_pred, train_true = [], []\n",
        "    model.train() \n",
        "    for idx, (data, targets, index) in enumerate(trainloader):\n",
        "        data, targets  = data.cuda(), targets.cuda()\n",
        "        y_pred = model(data)\n",
        "        y_prob = torch.sigmoid(y_pred)\n",
        "        loss = loss_fn(y_prob, targets, index_p=index) # make sure: index >0 for positive samples, and index < 0 for negative samples\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_pred.append(y_prob.cpu().detach().numpy())\n",
        "        train_true.append(targets.cpu().detach().numpy())\n",
        "\n",
        "    train_true = np.concatenate(train_true)\n",
        "    train_pred = np.concatenate(train_pred)\n",
        "    train_pauc = pauc_roc_score(train_true, train_pred, max_fpr=0.3)\n",
        "    train_list.append(train_pauc)\n",
        "    \n",
        "   # evaluation\n",
        "    model.eval()\n",
        "    test_pred, test_true = [], [] \n",
        "    for j, data in enumerate(testloader):\n",
        "        test_data, test_targets, index = data\n",
        "        test_data = test_data.cuda()\n",
        "        y_pred = model(test_data)\n",
        "        y_prob = torch.sigmoid(y_pred)\n",
        "        test_pred.append(y_prob.cpu().detach().numpy())\n",
        "        test_true.append(test_targets.numpy())\n",
        "    test_true = np.concatenate(test_true)\n",
        "    test_pred = np.concatenate(test_pred)\n",
        "    val_pauc = pauc_roc_score(test_true, test_pred, max_fpr=0.3)\n",
        "    test_list.append(val_pauc)\n",
        "    \n",
        "    if test_best < val_pauc:\n",
        "       test_best = val_pauc\n",
        "    \n",
        "    model.train()\n",
        "    print(\"epoch: %s, lr: %.4f, train_pauc: %.4f, test_pauc: %.4f, test_best: %.4f\"%(epoch, optimizer.lr, train_pauc, val_pauc, test_best))\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "11_Optimizing_pAUC_Loss_on_Imbalanced_data_wrapper.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}